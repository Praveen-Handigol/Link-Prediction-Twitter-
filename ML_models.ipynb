{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8612aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3682e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pars_train = pd.read_csv('Trainset_features_10000.csv')\n",
    "df_pars_test = pd.read_csv('Testset_features.csv')\n",
    "\n",
    "pars_train = np.array(df_pars_train)\n",
    "pars_test = np.array(df_pars_test)\n",
    "\n",
    "X = pars_train[:,3:]\n",
    "y = pars_train[:,2]\n",
    "\n",
    "X_testset = pars_test[:,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acedd745",
   "metadata": {},
   "source": [
    "Each feature accuracy\n",
    "3. Out degree source\n",
    "4. In degree source\n",
    "5. Out degree destination\n",
    "6. In degree destination\n",
    "7. Transitive friends\n",
    "8. Outdegree Common friends\n",
    "9. Indegree common friends\n",
    "10. Total friends\n",
    "11. Jaccard\n",
    "12. Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db16b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.150e+02 2.100e+01 0.000e+00 1.000e+00 2.360e+02]\n",
      " [3.010e+02 4.900e+01 0.000e+00 1.000e+00 3.500e+02]\n",
      " [7.940e+02 3.600e+01 0.000e+00 5.000e+00 4.150e+03]\n",
      " ...\n",
      " [3.800e+02 1.710e+02 0.000e+00 1.100e+01 6.061e+03]\n",
      " [4.300e+01 2.600e+01 0.000e+00 1.000e+00 6.900e+01]\n",
      " [5.660e+02 4.620e+02 0.000e+00 5.500e+01 5.654e+04]]\n"
     ]
    }
   ],
   "source": [
    "selected_columns = [3, 4, 5, 6, 13]\n",
    "\n",
    "X = pars_train[:,selected_columns]\n",
    "y = pars_train[:,2]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13742afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b2e105",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa5c3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# sc = StandardScaler()\n",
    "# sc.fit(X_train)\n",
    "# X_train_std = sc.transform(X_train)\n",
    "# X_test_std = sc.transform(X_test)\n",
    "# X_testset_std=sc.transform(X_testset)\n",
    "\n",
    "classifier = RandomForestRegressor(n_estimators = 1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred_RF = classifier.predict(X_test)\n",
    "y_pred_RF = np.where(y_pred_RF > 0.5, 1, 0)\n",
    "acc_RF =accuracy_score(y_test, y_pred_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c79d5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75325"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca5f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_RF_test = classifier.predict(X_testset_std)\n",
    "df_pred_RF_test = pd.DataFrame(y_pred_RF_test)\n",
    "df_pred_RF_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf7ba24",
   "metadata": {},
   "source": [
    "XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df35c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b25ef629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.791"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "imbalance_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "sample_weights = [2.0] * len(X_train)\n",
    "classifier_XGB = xgb.XGBClassifier(eta=0.01, scale_pos_weight=imbalance_ratio)\n",
    "classifier_XGB.fit(X_train, y_train)\n",
    "\n",
    "y_pred_XGB = classifier_XGB.predict(X_test)\n",
    "acc_XGB =accuracy_score(y_test, y_pred_XGB)\n",
    "acc_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c47152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_XGB = classifier_XGB.predict(X_testset)\n",
    "y_prob_XGB = classifier_XGB.predict_proba(X_testset_std)\n",
    "y_prob_class_1_XGB = y_prob_XGB[:, 1]\n",
    "#df_y_prob_XGB = pd.DataFrame(y_prob_class_1_XGB)\n",
    "#df_y_pred_XGB = pd.DataFrame(y_pred_XGB)\n",
    "y_prob_class_1_XGB\n",
    "# df_y_pred_XGB.to_csv('Test_submission_XGB_scaled_20000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f57ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_prob_XGB = pd.DataFrame(y_prob_class_1_XGB)\n",
    "df_y_prob_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f405440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_y_prob_XGB.to_csv('Test_submission_XGB_20000_0110.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce3571",
   "metadata": {},
   "source": [
    "ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f928fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5c27b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.791\n"
     ]
    }
   ],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "# Train Adaboost Classifer\n",
    "model = abc.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_AB = model.predict(X_test)\n",
    "print(\"Accuracy score %.3f\" %metrics.accuracy_score(y_test, y_pred_AB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda9768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_AB =abc.predict(X_testset)\n",
    "y_prob_AB = model.predict_proba(X_testset_std)\n",
    "y_prob_class_1_AB = y_prob_AB[:, 1]\n",
    "df_y_pred_AB = pd.DataFrame(y_prob_class_1_AB)\n",
    "df_y_pred_AB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b491a",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5281cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2ed202e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 11 features, but StandardScaler is expecting 5 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m X_train_std_SVM \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtransform(X_train)\n\u001b[0;32m      4\u001b[0m X_test_std_SVM \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m----> 5\u001b[0m X_testset_std_SVM\u001b[38;5;241m=\u001b[39msc\u001b[38;5;241m.\u001b[39mtransform(X_testset)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1004\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1001\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1003\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1004\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1005\u001b[0m     X,\n\u001b[0;32m   1006\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1007\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1008\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1009\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m   1010\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1011\u001b[0m )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:625\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:414\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 11 features, but StandardScaler is expecting 5 features as input."
     ]
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std_SVM = sc.transform(X_train)\n",
    "X_test_std_SVM = sc.transform(X_test)\n",
    "X_testset_std_SVM=sc.transform(X_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "511553b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, probability=True, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, probability=True, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=10, probability=True, random_state=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the Support Vector Classifier (SVC)\n",
    "svc = SVC(C=10, random_state=1, kernel='rbf', probability=True)\n",
    " \n",
    "# Fit the model\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7f95503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.664\n"
     ]
    }
   ],
   "source": [
    "y_pred_SVM = svc.predict(X_test)\n",
    " \n",
    "# Measure the performance\n",
    "print(\"Accuracy score %.3f\" %metrics.accuracy_score(y_test, y_pred_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7129c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_SVM = svc.predict(X_testset_std_SVM)\n",
    "y_prob_SVM = svc.predict_proba(X_testset_std_SVM)\n",
    "y_prob_class_1_SVM = y_prob_SVM[:, 1]\n",
    "df_y_pred_SVM = pd.DataFrame(y_prob_class_1_SVM)\n",
    "df_y_pred_SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca1a1d6",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0975fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn_model = KNeighborsRegressor(n_neighbors=150)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03102cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78775"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model.fit(X_train, y_train)\n",
    "y_pred_KNN = knn_model.predict(X_test)\n",
    "#mse = mean_squared_error(y_train, y_pred_KNN)\n",
    "train_preds1 = np.where(y_pred_KNN > 0.5, 1, 0)\n",
    "acc_KNN =accuracy_score(y_test,train_preds1)\n",
    "acc_KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb56ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_KNN = knn_model.predict(X_testset)\n",
    "df_y_pred_KNN = pd.DataFrame(y_pred_KNN)\n",
    "#df_y_pred_KNN.to_csv('Test_submission_LR_20000.csv', index=False)\n",
    "df_y_pred_KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = np.zeros(2)\n",
    "zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173573b6",
   "metadata": {},
   "source": [
    "NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d275b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63875"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "NaiveBayes_clf = BernoulliNB()\n",
    "NaiveBayes_clf.fit(X_train, y_train)\n",
    "y_pred_NB = NaiveBayes_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_NB = NaiveBayes_clf.predict(X_testset)\n",
    "log_probabilities = NaiveBayes_clf.predict_log_proba(X_testset)\n",
    "probabilities = np.exp(log_probabilities)\n",
    "y_prob_class_1_NB = probabilities[:, 1]\n",
    "df_y_prob_NB = pd.DataFrame(y_prob_class_1_NB)\n",
    "df_y_prob_NB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb5713",
   "metadata": {},
   "source": [
    "Logistic Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14c10e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_LR = LogisticRegression(solver= 'liblinear', random_state=0)\n",
    "model_LR.fit(X_train, y_train)\n",
    "y_predict_LR = model_LR.predict(X_test)\n",
    "\n",
    "\n",
    " \n",
    "# Measure the performance\n",
    "print(\"Accuracy score %.3f\" %metrics.accuracy_score(y_test, y_predict_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_LR = model_LR.predict(X_testset_std)\n",
    "y_prob_LR = model_LR.predict_proba(X_testset_std)\n",
    "y_prob_class_1_LR = y_prob_LR[:, 1]\n",
    "df_y_pred_LR = pd.DataFrame(y_prob_class_1_LR)\n",
    "df_y_pred_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3409aa3e",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dae5a35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "500/500 [==============================] - 1s 901us/step - loss: 1102.1115 - accuracy: 0.7013\n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 0s 867us/step - loss: 726.3911 - accuracy: 0.7114\n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 0s 865us/step - loss: 1053.8536 - accuracy: 0.7299\n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 0s 864us/step - loss: 327.7672 - accuracy: 0.7511\n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 0s 856us/step - loss: 376.3946 - accuracy: 0.7422\n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 0s 877us/step - loss: 579.5347 - accuracy: 0.7134\n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 0s 860us/step - loss: 561.9406 - accuracy: 0.7517\n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 0s 872us/step - loss: 594.0258 - accuracy: 0.7488\n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 0s 865us/step - loss: 232.8938 - accuracy: 0.7468\n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 0s 870us/step - loss: 349.8885 - accuracy: 0.7299\n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 0s 897us/step - loss: 141.7476 - accuracy: 0.7375\n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 0s 973us/step - loss: 202.2424 - accuracy: 0.7431\n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 0s 874us/step - loss: 149.3887 - accuracy: 0.7509\n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 0s 860us/step - loss: 144.6210 - accuracy: 0.7479\n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 0s 861us/step - loss: 35.8111 - accuracy: 0.7416\n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 0s 868us/step - loss: 65.4942 - accuracy: 0.7382\n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 0s 858us/step - loss: 25.9716 - accuracy: 0.7373\n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 0s 950us/step - loss: 48.5467 - accuracy: 0.7268\n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 0s 964us/step - loss: 65.0341 - accuracy: 0.7330\n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 0s 909us/step - loss: 68.1797 - accuracy: 0.7238\n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 0s 937us/step - loss: 32.0909 - accuracy: 0.7623\n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 0s 936us/step - loss: 18.4489 - accuracy: 0.7555\n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 0s 949us/step - loss: 20.5959 - accuracy: 0.7618\n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 0s 917us/step - loss: 9.9703 - accuracy: 0.7439\n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 0s 910us/step - loss: 24.1829 - accuracy: 0.7449\n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 0s 883us/step - loss: 10.5263 - accuracy: 0.7645\n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 0s 874us/step - loss: 27.0810 - accuracy: 0.7491\n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 0s 881us/step - loss: 12.0756 - accuracy: 0.7608\n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 0s 883us/step - loss: 7.5711 - accuracy: 0.7530\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 0s 892us/step - loss: 5.6932 - accuracy: 0.7604\n",
      "125/125 [==============================] - 0s 721us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.787"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# sc = StandardScaler()\n",
    "# sc.fit(X_train)\n",
    "# X_train_std = sc.transform(X_train)\n",
    "# X_test_std = sc.transform(X_test)\n",
    "# X_testset_std=sc.transform(X_testset)\n",
    "\n",
    "# ann = tf.keras.Sequential()\n",
    "# ann.add(tf.keras.layers.Dense(units=10,activation='relu'))\n",
    "# ann.add(tf.keras.layers.Dense(units=64,activation='relu'))\n",
    "# ann.add(tf.keras.layers.Dense(units=32,activation='relu'))\n",
    "# ann.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))\n",
    "# ann.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "ann = tf.keras.Sequential()\n",
    "\n",
    "# Add the first hidden layer with Leaky ReLU activation\n",
    "ann.add(tf.keras.layers.Dense(units=11, activation=tf.nn.leaky_relu))\n",
    "\n",
    "# Add the second hidden layer with PReLU activation\n",
    "ann.add(tf.keras.layers.Dense(units=128))\n",
    "ann.add(tf.keras.layers.PReLU())\n",
    "\n",
    "# Add the third hidden layer with PReLU activation\n",
    "ann.add(tf.keras.layers.Dense(units=32))\n",
    "ann.add(tf.keras.layers.PReLU())\n",
    "\n",
    "# Add the output layer with sigmoid activation\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "ann.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ann.fit(X_train,y_train,batch_size=32,epochs = 30 )\n",
    "y_pred_ANN = ann.predict(X_test)\n",
    "y_pred_ANN = np.where(y_pred_ANN > 0.5, 1, 0)\n",
    "accuracy_score(y_test, y_pred_ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c57b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ANN =ann.predict(X_testset_std)\n",
    "df_y_pred_ANN = pd.DataFrame(y_pred_ANN)\n",
    "#df_y_pred_ANN.to_csv('Test_Submission_20000_ANN_0110_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a44db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_probs = np.column_stack((y_pred_ANN, y_prob_class_1_LR, y_prob_class_1_NB, y_pred_KNN, y_prob_class_1_SVM, y_prob_class_1_AB, y_prob_class_1_XGB,y_pred_RF_test))\n",
    "average_probs = np.mean(ensemble_probs, axis=1)\n",
    "df_y_pred_X_testset_sub = pd.DataFrame(average_probs)\n",
    "#df_y_pred_X_testset_sub.to_csv('Test_submission_Ensemble_20000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07702190",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_probs = np.median(ensemble_probs, axis=1)\n",
    "df_y_pred_X_testset_sub = pd.DataFrame(median_probs)\n",
    "#df_y_pred_X_testset_sub.to_csv('Test_submission_Ensemble_Median_20000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow scikeras scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ad4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ann_model():\n",
    "    model = Sequential()\n",
    "    # Add your layers here\n",
    "    model.add(Dense(units=10, activation=tf.nn.leaky_relu, input_dim=X_train.shape[1]))\n",
    "    model.add(Dense(units=128, activation=tf.nn.leaky_relu))\n",
    "    model.add(Dense(units=32, activation=tf.nn.leaky_relu))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "ann_model = KerasClassifier(build_fn=create_ann_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20371028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def create_ann_model():\n",
    "    model = Sequential()\n",
    "    # Add your layers here\n",
    "    model.add(Dense(units=11, activation=tf.nn.leaky_relu, input_dim=X_train.shape[1]))\n",
    "    model.add(Dense(units=128, activation=tf.nn.leaky_relu))\n",
    "    model.add(Dense(units=32, activation=tf.nn.leaky_relu))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize each individual model (including the Keras model wrapped in KerasClassifier)\n",
    "ann_model = KerasClassifier(build_fn=create_ann_model, epochs=20, batch_size=32)\n",
    "svm_model = SVC(C=10, random_state=1, kernel='rbf', probability=True)\n",
    "rf_model = RandomForestClassifier(n_estimators = 1000)\n",
    "lr_model = LogisticRegression(solver= 'liblinear', random_state=0)\n",
    "xgb_model = XGBClassifier(eta=0.01, scale_pos_weight=imbalance_ratio)\n",
    "ada_model = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "nb_model = BernoulliNB()\n",
    "knn_model = KNeighborsClassifier(n_neighbors=150)\n",
    "\n",
    "# Create a Voting Classifier\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('ann', ann_model),\n",
    "    ('svm', svm_model),\n",
    "    ('rf', rf_model),\n",
    "    ('lr', lr_model),\n",
    "    ('xgb', xgb_model),\n",
    "    ('ada', ada_model),\n",
    "    ('nb', nb_model),\n",
    "    ('knn',knn_model)\n",
    "], voting='soft')  # 'soft' uses probability estimates for voting\n",
    "\n",
    "voting_classifier.fit(X_train_std, y_train)\n",
    "\n",
    "# Make predictions using the ensemble model\n",
    "ensemble_predictions = voting_classifier.predict(X_test_std)\n",
    "ensemble_predictions = np.where(ensemble_predictions > 0.5, 1, 0)\n",
    "\n",
    "# Evaluate the ensemble's performance\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(f'Ensemble Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pars_test = pd.read_csv('Testset_features.csv')\n",
    "pars_test = np.array(df_pars_test)\n",
    "pars_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55402a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testset = pars_test[:,2:]\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_testset)\n",
    "X_testsub_std = sc.transform(X_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7863e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = voting_classifier.predict(X_testsub_std)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d3586",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = pd.DataFrame(y_pred_test)\n",
    "#Ans.to_csv('Test_Submission_20000_Ens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98987083",
   "metadata": {},
   "outputs": [],
   "source": [
    "## leaky relu, accuracy 0.8282 with one hidden layer and 32\n",
    "## scaled data, leaky relu, acc 0.8308, 2 hidden layers and 64,32\n",
    "## scaled data, relu, acc 0.831, 2 hidden layers and 64,32\n",
    "## scaled data, leaky relu, acc 0.8308, 2 hidden layers and 64,32\n",
    "## scaled data, (LR(i/p) PRelu prelu), acc 0.8348, 2 hidden layers and 128,64\n",
    "## scaled data, (LR(i/p) PRelu prelu), acc 0.8288, 2 hidden layers and 128,128\n",
    "## scaled data, (LR(i/p) PRelu prelu), acc 0.8348, 2 hidden layers and 128,64\n",
    "scaled data, (LR(input) PRelu prelu), acc 0.8358, 2 hidden layers and 128,32\n",
    "## scaled data, (LR(i/p) PRelu prelu), acc 0.8308, 2 hidden layers and 128,16\n",
    "## scaled data, (elu(i/p) elu elu), acc 0.8328, 2 hidden layers and 64,32\n",
    "## scaled data, (elu(i/p) elu elu), acc 0.8326, 2 hidden layers and 128,32\n",
    "scaled data, (elu(i/p) elu elu), acc 0.8358, 2 hidden layers and 128,64\n",
    "## scaled data, (elu(i/p) elu elu), acc 0.8304, 2 hidden layers and 128,128\n",
    "## scaled data, (selu(i/p) selu selu), acc 0.8318, 2 hidden layers and 64,32\n",
    "## scaled data, (selu(i/p) selu selu), acc 0.829, 2 hidden layers and 128,32\n",
    "## scaled data, (selu(i/p) selu selu), acc 0.831, 2 hidden layers and 64,64\n",
    "## scaled data, (selu(i/p) selu selu), acc 0.834, 2 hidden layers and 64,16\n",
    "## scaled data, (selu(i/p) selu selu), acc 0.829, 2 hidden layers and 64,8\n",
    "## scaled data, (crelu(i/p) crelu crelu), acc 0.8344, 2 hidden layers and 64,32\n",
    "## scaled data, (crelu(i/p) crelu crelu), acc 0.8252, 2 hidden layers and 64,64\n",
    "## scaled data, (crelu(i/p) crelu crelu), acc 0.8342, 2 hidden layers and 64,16\n",
    "## scaled data, (crelu(i/p) crelu crelu), acc 0.832, 2 hidden layers and 128,64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d293577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = tf.keras.Sequential()\n",
    "\n",
    "# Add the first hidden layer with Leaky ReLU activation\n",
    "ann.add(tf.keras.layers.Dense(units=10, activation=tf.nn.leaky_relu))\n",
    "\n",
    "# Add the second hidden layer with PReLU activation\n",
    "ann.add(tf.keras.layers.Dense(units=128))\n",
    "ann.add(tf.keras.layers.PReLU())\n",
    "\n",
    "# Add the third hidden layer with PReLU activation\n",
    "ann.add(tf.keras.layers.Dense(units=30))\n",
    "ann.add(tf.keras.layers.PReLU())\n",
    "\n",
    "# Add the output layer with sigmoid activation\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "ann.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b63da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a Sequential model\n",
    "ann = tf.keras.Sequential()\n",
    "\n",
    "# Add the first hidden layer with ELU activation\n",
    "ann.add(tf.keras.layers.Dense(units=10, activation='elu'))\n",
    "\n",
    "# Add the second hidden layer with ELU activation\n",
    "ann.add(tf.keras.layers.Dense(units=128, activation='elu'))\n",
    "\n",
    "# Add the third hidden layer with ELU activation\n",
    "ann.add(tf.keras.layers.Dense(units=128, activation='elu'))\n",
    "\n",
    "# Add the output layer with sigmoid activation\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a Sequential model\n",
    "ann = tf.keras.Sequential()\n",
    "\n",
    "# Add the first hidden layer with SELU activation\n",
    "ann.add(tf.keras.layers.Dense(units=10, activation='selu'))\n",
    "\n",
    "# Add the second hidden layer with SELU activation\n",
    "ann.add(tf.keras.layers.Dense(units=64, activation='selu'))\n",
    "\n",
    "# Add the third hidden layer with SELU activation\n",
    "ann.add(tf.keras.layers.Dense(units=8, activation='selu'))\n",
    "\n",
    "# Add the output layer with sigmoid activation\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7bd9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom RReLU activation layer\n",
    "class RReLU(tf.keras.layers.Layer):\n",
    "    def __init__(self, lower=0.225, upper=0.433):\n",
    "        super(RReLU, self).__init__()\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "\n",
    "    def call(self, inputs):\n",
    "        alpha = tf.random.uniform(shape=tf.shape(inputs), minval=self.lower, maxval=self.upper)\n",
    "        return tf.maximum(alpha * inputs, inputs)\n",
    "\n",
    "# Create a Sequential model\n",
    "ann = tf.keras.Sequential()\n",
    "\n",
    "# Add the first hidden layer with RReLU activation\n",
    "ann.add(tf.keras.layers.Dense(units=10))\n",
    "ann.add(RReLU())\n",
    "\n",
    "# Add the second hidden layer with RReLU activation\n",
    "ann.add(tf.keras.layers.Dense(units=128))\n",
    "ann.add(RReLU())\n",
    "\n",
    "# Add the third hidden layer with RReLU activation\n",
    "ann.add(tf.keras.layers.Dense(units=32))\n",
    "ann.add(RReLU())\n",
    "\n",
    "# Add the output layer with sigmoid activation\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Concatenate\n",
    "\n",
    "# Define a custom CReLU layer\n",
    "class CReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super(CReLU, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        positive = tf.keras.activations.relu(inputs)\n",
    "        negative = tf.keras.activations.relu(-inputs)\n",
    "        return Concatenate()([positive, negative])\n",
    "\n",
    "# Create a Sequential model\n",
    "ann = tf.keras.Sequential()\n",
    "\n",
    "# Add the first hidden layer with CReLU activation\n",
    "ann.add(tf.keras.layers.Dense(units=10, activation=CReLU()))\n",
    "\n",
    "# Add the second hidden layer with CReLU activation\n",
    "ann.add(tf.keras.layers.Dense(units=64, activation=CReLU()))\n",
    "\n",
    "# Add the third hidden layer with CReLU activation\n",
    "ann.add(tf.keras.layers.Dense(units=16, activation=CReLU()))\n",
    "\n",
    "# Add the output layer with sigmoid activation\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
